\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation}
\author{Christopher Oldham, Linfei Xie, Syed Ali Asar }
\date{May 2019}

\begin{document}

\maketitle

\begin{abstract}
    Generating de novo large, complex graphs that satisfy specific properties can be a difficult task, especially if the rules governing the graphs are very complex and nonlinear such as for drug-like molecular graphs. The space of possible drug-like molecular graphs is massive, and hence search algorithms are unfeasible: a generative model is necessary. Ying, Pande and Leskovec have developed such a generative algorithm in their paper \textit{Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation}. This algorithm combines reinforcement learning (RL) and generative adversarial network (GAN) paradigms to generate novel drug-like molecular graphs. This algorithm trains an actor to build molecular graphs, atom by atom to fulfill certain chemical goals as well as similarity to training examples. We set up an environment to run their algorithm, and trained the generation of novel drug-like molecules using the ZINC 250k dataset. While we did not have the computing resources for the model to fully converge, we did generate never-before-seen drug-like molecules.bluet
\end{abstract}

\section{Theoretical Background}
In some domains, novel graphs can be created by heuristic processes, iterative improvement (such as edge pruning) or even searching over the entire domain of graphs. However for some more complex domains such as molecule generation these methods will not suffice. To begin, the rules for desirable molecules are too complicated for heuristic enumeration, an some properties such as druglikeness are not very well defined from a molecular perspective. Due to the discrete nature of molecules and the ultimately quantum-mechanical rules that govern them, these rules would be incredibly complex and nonlinear even if they could be fully described.

Furthermore, the global properties of molecules are very sensitive to small changes in structure, and the search space for possible drug-like molecules may include up to $10^{60}$ candidates. It would therefore be impossible to search through all possible molecular graphs and evaluate them post hoc. It would also be impossible to accurately enumerate heuristics a priori for the generation of drug-like molecules. Therefore Ying, Pande and Leskovec at Stanford University developed the Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation.

This approach sees molecular graph generation as a reinforcement learning problem where an actor selects what atom to bond, where, and how to a scaffold molecule. The policy for action selection is partially informed by domain specific properties and partially by an adversarial reward. The adversarial reward promotes the generation of molecules that have properties similar to molecules from a large dataset of known drug-like molecules. This reward approach draws on the very powerful generative adversarial network (GAN) paradigm which has seen large success in producing novel images. The domain specific reward comes from various chemical properties such as synthetic accessibility. Rewards are given for each atomic action, as well as for complete molecules.

While the work is largely specific to molecule generation, the algorithm is designed to be adaptable to a range of graph generation problems for which many examples exist and specific desirable properties are well defined. The authors of the paper suggest possible applications in biological and social networks.
In our work we only consider the domain of drug-like molecular graphs. There is a large dataset (ZINC 250k) for adversarial training. Furthermore it is the example used by the authors, and is arguably the most interesting domain for current reseasrch.
\subsection{Molecular Graphs}
\subsection{Markov Decision Processes}
\subsection{Reinforcement Learning}
\subsection{Generative Adversarial Networks}
\subsection{Chemical Details}
\subsubsection*{Valency}
\subsubsection*{Druglikeness}
\subsubsection*{Steric Strain}
\subsubsection*{Synthetic Availability}
\section{Implementation}
\subsection{OpenAI Gym}
\subsection{RDKit}
\subsection{etc.}
\section{Technical Details}
\subsection{ZINC250k Dataset}
\subsection{Operating Environment and Program Time}
\section{Results}
\section{Discussion}

\end{document}
